{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee666ea2-2d00-45eb-84c7-8f47a2c187aa",
   "metadata": {},
   "source": [
    "# msaexp reduction\n",
    "\n",
    "The first step is a standard msaexp reduction.\n",
    "\n",
    "This code depends on grizli, jwst, msaexp packages.\n",
    "\n",
    "I assume that you have obtained *_rate.fits files by running the jwst pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5729ef8-baf8-4d75-adc1-8aced808aacb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'grizli'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRDS_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/myue/Research/Projects/JWST/dependencies/crds_cache/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRDS_SERVER_URL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://jwst-crds.stsci.edu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgrizli\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jwst_level1\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmsaexp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmastquery\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjwst\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'grizli'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Quiet!\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CRDS_PATH\"] = \"/Users/myue/Research/Projects/JWST/dependencies/crds_cache/\"\n",
    "os.environ[\"CRDS_SERVER_URL\"] = \"https://jwst-crds.stsci.edu\"\n",
    "\n",
    "from grizli import jwst_level1\n",
    "from msaexp import pipeline\n",
    "import mastquery.jwst\n",
    "import mastquery.utils\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "\n",
    "import astropy.time\n",
    "\n",
    "import grizli\n",
    "from grizli import utils, jwst_utils\n",
    "jwst_utils.set_quiet_logging()\n",
    "utils.set_warnings()\n",
    "\n",
    "import astropy.io.fits as pyfits\n",
    "from jwst.datamodels import SlitModel\n",
    "\n",
    "import msaexp\n",
    "from msaexp import pipeline\n",
    "try:\n",
    "    from msaexp import pipeline_extended\n",
    "    HAS_EXTENDED_PIPELINE = True\n",
    "except:\n",
    "    HAS_EXTENDED_PIPELINE = False\n",
    "\n",
    "import jwst\n",
    "import time\n",
    "\n",
    "#NIRSPEC_HOME = '/Users/jmatthee/Documents/storage/NIRspec/'\n",
    "HOME = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd90903-3abe-4552-b185-ceb1a5727982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions are from Jorryt.\n",
    "# I will clean it up later to make it easier to use.\n",
    "\n",
    "def new_filename(rate_file='jw01286005001_03101_00002_nrs2_rate.fits', c='b'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    spl = rate_file.split('_')\n",
    "    spl[2] = c + spl[2][1:]\n",
    "    return '_'.join(spl)\n",
    "\n",
    "\n",
    "def preprocess_nirspec_file(rate_file='jw01286005001_03101_00002_nrs2_rate.fits', root='jades-gds05-v3', as_fixed=False, rename_f070=False, context='jwst_1225.pmap', clean=True, extend_wavelengths=True, undo_flat=True, by_source=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Run preprocessing calibrations for a single NIRSpec exposure\n",
    "    \"\"\"\n",
    "    from grizli import jwst_level1\n",
    "\n",
    "    os.environ['CRDS_CONTEXT'] = os.environ['CRDS_CTX'] = context\n",
    "    jwst_utils.set_crds_context()\n",
    "\n",
    "    # print(rate_file, root)\n",
    "\n",
    "    outroot = root\n",
    "\n",
    "    if extend_wavelengths:\n",
    "        rename_f070 = False\n",
    "\n",
    "    file_prefix = rate_file.split('_rate')[0]\n",
    "    # key = root + '-' + file_prefix\n",
    "    key = f'{root}-{file_prefix}-{rename_f070}'\n",
    "\n",
    "    WORKPATH = os.path.join(HOME, key)\n",
    "\n",
    "    OUTPUT_PATH = f'{NIRSPEC_HOME}/{outroot}'\n",
    "\n",
    "    if not os.path.exists(WORKPATH):\n",
    "        os.makedirs(WORKPATH)\n",
    "\n",
    "    os.chdir(WORKPATH)\n",
    "\n",
    "    _ORIG_LOGFILE = utils.LOGFILE\n",
    "    _NEW_LOGFILE = os.path.join(WORKPATH, file_prefix + '_rate.log.txt')\n",
    "    utils.LOGFILE = _NEW_LOGFILE\n",
    "\n",
    "    msg = f\"\"\"# {rate_file} {root}\n",
    "  jwst version = {jwst.__version__}\n",
    "grizli version = {grizli.__version__}\n",
    "msaexp version = {msaexp.__version__}\n",
    "    \"\"\"\n",
    "    utils.log_comment(utils.LOGFILE, msg, verbose=True)\n",
    "\n",
    "    # Download to working directory\n",
    "    mastquery.utils.download_from_mast([rate_file], overwrite=False)\n",
    "\n",
    "    os.system(f'aws s3 cp s3://grizli-v2/reprocess_rate/{rate_file} .')\n",
    "\n",
    "    if not os.path.exists(rate_file):\n",
    "        msg = f\"Failed to download {rate_file}\"\n",
    "        utils.log_comment(utils.LOGFILE, msg, verbose=True)\n",
    "        return 3\n",
    "\n",
    "    if not as_fixed:\n",
    "        with pyfits.open(rate_file) as im:\n",
    "            if 'MSAMETFL' in im[0].header:\n",
    "                msametf = im[0].header['MSAMETFL']\n",
    "                mastquery.utils.download_from_mast([msametf], overwrite=False)\n",
    "                \n",
    "                msa = msaexp.msa.MSAMetafile(msametf)\n",
    "                msa.merge_source_ids()\n",
    "                msa.write(prefix='', overwrite=True)\n",
    "                \n",
    "            else:\n",
    "                msametf = None\n",
    "    else:\n",
    "        msametf = None\n",
    "        by_source = False\n",
    "\n",
    "    use_file = rate_file\n",
    "\n",
    "    if rename_f070:\n",
    "        with pyfits.open(rate_file) as im:\n",
    "            filt = im[0].header['FILTER']\n",
    "            if filt == 'F070LP':\n",
    "                im[0].header['FILTER'] = 'F100LP'\n",
    "\n",
    "                new_file = new_filename(rate_file, c='b')\n",
    "                msg = \"Rename blocking filter F070LP to F100LP\\n\"\n",
    "                msg += f\"Rename {rate_file} > {new_file}\"\n",
    "\n",
    "                im.writeto(new_file, overwrite=True)\n",
    "\n",
    "                os.remove(rate_file)\n",
    "\n",
    "                use_file = new_file\n",
    "\n",
    "                utils.log_comment(utils.LOGFILE, msg, verbose=True)\n",
    "\n",
    "    use_prefix = use_file.split('_rate')[0]\n",
    "\n",
    "    files = [use_file]\n",
    "    files.sort()\n",
    "\n",
    "    utils.log_comment(utils.LOGFILE, 'Reset DQ=4 flags', verbose=True)\n",
    "\n",
    "    for _file in files:\n",
    "        with pyfits.open(_file, mode='update') as im:\n",
    "            # print(f'_file unset DQ=4')\n",
    "            im['DQ'].data -= im['DQ'].data & 4\n",
    "            im.flush()\n",
    "\n",
    "    # Split into groups of 3 exposures\n",
    "    groups = pipeline.exposure_groups(files=files, split_groups=True)\n",
    "\n",
    "    print('Files:')\n",
    "    print('======')\n",
    "    print(yaml.dump(dict(groups)))\n",
    "\n",
    "    # Single exposure groups\n",
    "    single_exposure_groups = {}\n",
    "\n",
    "    for g in groups:\n",
    "        for exp, k in zip(groups[g], 'abcdefghijklmnopqrstuvwxyz'[:len(groups[g])]):\n",
    "            gr = g.replace('-f', f'{k}-f').replace('-clear', f'{k}-clear')\n",
    "            single_exposure_groups[gr] = [exp]\n",
    "\n",
    "    print(yaml.dump(dict(single_exposure_groups)))\n",
    "\n",
    "    pipes = []\n",
    "\n",
    "    from jwst.assign_wcs.util import NoDataOnDetectorError\n",
    "\n",
    "    source_ids = None\n",
    "\n",
    "    pad = 0\n",
    "\n",
    "    positive = False\n",
    "    sources = None\n",
    "\n",
    "    # Should just be one group....\n",
    "    for g in groups:\n",
    "        for exp, k in zip(groups[g], 'abcdefghijklmnopqrstuvwxyz'[:len(groups[g])]):\n",
    "            mode = g.replace('-f', f'{k}-f').replace('-clear', f'{k}-clear')\n",
    "            xmode = f'{mode}-fixed' if as_fixed else mode\n",
    "\n",
    "            if sources is not None:\n",
    "                source_ids = sources[g] #[3:6]\n",
    "                if len(source_ids) < 1:\n",
    "                    source_ids = None\n",
    "            else:\n",
    "                source_ids = None\n",
    "\n",
    "            if os.path.exists(f'{xmode}.start'):\n",
    "                print(f'Already started: {mode}')\n",
    "                continue\n",
    "\n",
    "            if outroot in ['macs0417-v1']:\n",
    "                source_ids = None\n",
    "                positive = False\n",
    "\n",
    "            source_ids = None\n",
    "            positive = False\n",
    "\n",
    "            if not os.path.exists(f'{xmode}.slits.yaml'):#\n",
    "                with open(f'{xmode}.start','w') as fp:\n",
    "                    fp.write(time.ctime())\n",
    "\n",
    "                if 0:\n",
    "                    source_ids = sources[mode]\n",
    "\n",
    "                if as_fixed:\n",
    "                    for _file in single_exposure_groups[mode]:\n",
    "                        with pyfits.open(_file, mode='update') as _im:\n",
    "                            ORIG_EXPTYPE = _im[0].header['EXP_TYPE']\n",
    "                            if ORIG_EXPTYPE != 'NRS_FIXEDSLIT':\n",
    "                                print(f'Set {_file} MSA > FIXEDSLIT keywords')\n",
    "                                _im[0].header['EXP_TYPE'] = 'NRS_FIXEDSLIT'\n",
    "                                _im[0].header['APERNAME'] = 'NRS_S200A2_SLIT'\n",
    "                                _im[0].header['OPMODE'] = 'FIXEDSLIT'\n",
    "                                _im[0].header['FXD_SLIT'] = 'S200A2'\n",
    "                                _im.flush()\n",
    "\n",
    "                if extend_wavelengths:\n",
    "\n",
    "                    if by_source & (msametf is not None):\n",
    "                        # Run by individual source IDs\n",
    "                        rate_file = single_exposure_groups[mode][0]\n",
    "\n",
    "                        msa = msaexp.msa.MSAMetafile(msametf)\n",
    "                        msa.merge_source_ids()\n",
    "                        msa.write(prefix='', overwrite=True)\n",
    "\n",
    "                        source_ids = msaexp.msa.get_msa_source_ids(rate_file)\n",
    "\n",
    "                        with pyfits.open(rate_file, mode='update') as im:\n",
    "                            if 'src' not in im[0].header['MSAMETFL']:\n",
    "                                im[0].header['MSAMETFL'] = 'src_' + msametf\n",
    "\n",
    "                            im.flush()\n",
    "\n",
    "                        # Run by source_id\n",
    "                        for source_id in source_ids:\n",
    "                            done_files = glob.glob(f\"*_{source_id}.fits\")\n",
    "                            if len(done_files) > 0:\n",
    "                                print(f'Skip completed {done_files[0]}')\n",
    "\n",
    "                            msametfl = msaexp.msa.pad_msa_metafile(\n",
    "                                msametf,\n",
    "                                pad=0,\n",
    "                                positive_ids=True,\n",
    "                                source_ids=[source_id],\n",
    "                                slitlet_ids=None,\n",
    "                                primary_sources=True,\n",
    "                            )\n",
    "\n",
    "                            try:\n",
    "                                pipe = pipeline_extended.run_pipeline(\n",
    "                                    rate_file,\n",
    "                                    slit_index=0,\n",
    "                                    all_slits=True,\n",
    "                                    write_output=True,\n",
    "                                    set_log=True,\n",
    "                                    skip_existing_log=False,\n",
    "                                    undo_flat=undo_flat,\n",
    "                                )\n",
    "                            except ValueError:\n",
    "                                msg = f'Failed to process source_id={source_id}'\n",
    "                                utils.log_comment(utils.LOGFILE, msg, verbose=True)\n",
    "                                continue\n",
    "\n",
    "                        # Set it back\n",
    "                        with pyfits.open(rate_file, mode='update') as im:\n",
    "                            if 'src' not in im[0].header['MSAMETFL']:\n",
    "                                im[0].header['MSAMETFL'] = msametf\n",
    "\n",
    "                            im.flush()\n",
    "\n",
    "                    else:\n",
    "                        pipe = pipeline_extended.run_pipeline(\n",
    "                            single_exposure_groups[mode][0],\n",
    "                            slit_index=0,\n",
    "                            all_slits=True,\n",
    "                            write_output=True,\n",
    "                            set_log=True,\n",
    "                            skip_existing_log=False,\n",
    "                            undo_flat=undo_flat,\n",
    "                        )\n",
    "\n",
    "                        if as_fixed:\n",
    "                            photom_file = f\"{use_prefix}_fs-photom.fits\"\n",
    "                        else:\n",
    "                            photom_file = f\"{use_prefix}_photom.fits\"\n",
    "                        \n",
    "                        print(f'Write {photom_file}')\n",
    "                        pipe.write(photom_file)\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        pipe = pipeline.NirspecPipeline(mode=xmode,\n",
    "                                                    files=single_exposure_groups[mode],\n",
    "                                                    source_ids=source_ids,\n",
    "                                                    pad=pad,\n",
    "                                                    positive_ids=positive # Ignore background slits\n",
    "                                                   )\n",
    "\n",
    "                        pipe.full_pipeline(run_extractions=False,\n",
    "                                           initialize_bkg=False,\n",
    "                                           load_saved=None,\n",
    "                                           scale_rnoise=False,\n",
    "                                           fix_rows=False,\n",
    "                                           )\n",
    "\n",
    "                    except NoDataOnDetectorError:\n",
    "                        print('NoDataOnDetectorError - skip')\n",
    "                        pipe = None\n",
    "                    # continue\n",
    "\n",
    "                if as_fixed:\n",
    "                    for _file in single_exposure_groups[mode]:\n",
    "                        with pyfits.open(_file, mode='update') as _im:\n",
    "                            if ORIG_EXPTYPE == 'NRS_MSASPEC':\n",
    "                                print(f'Reset {_file} FIXEDSLIT > MSA keywords')\n",
    "                                _im[0].header['EXP_TYPE'] = 'NRS_MSASPEC'\n",
    "                                _im[0].header['APERNAME'] = 'NRS_FULL_MSA'\n",
    "                                _im[0].header['OPMODE'] = 'MSASPEC'\n",
    "                                _im[0].header.pop('FXD_SLIT')\n",
    "                                _im.flush()\n",
    "\n",
    "                # pipes.append(pipe)\n",
    "                del(pipe)\n",
    "\n",
    "                os.remove(f'{xmode}.start')\n",
    "                print(f'{xmode} - Done! {time.ctime()}')\n",
    "\n",
    "            else:\n",
    "                print(f'Already completed: {mode}')\n",
    "\n",
    "            os.system(f'cat {mode}.log.txt >> {_NEW_LOGFILE}')\n",
    "\n",
    "            # break\n",
    "\n",
    "    utils.LOGFILE = _NEW_LOGFILE\n",
    "\n",
    "    # Sync slitlets to S3\n",
    "    if outroot.split('-')[0] in ['macs0417','macs1423','macs0416','abell370']:\n",
    "        s3path = 'grizli-canucs/nirspec'\n",
    "    else:\n",
    "        s3path = 'msaexp-nirspec/extractions'\n",
    "\n",
    "    if (outroot not in ['uncover-deep-v1']) & (1):\n",
    "        msg = f'Sync slitlets to s3://{s3path}/slitlets/{outroot}/'\n",
    "        utils.log_comment(utils.LOGFILE, msg, verbose=True)\n",
    "\n",
    "        os.system(f'aws s3 sync ./ s3://{s3path}/slitlets/{outroot}/ --exclude \"*\" --include \"*phot.*\" --include \"*raw.*\" --include \"*photom.*\" --acl public-read --quiet')\n",
    "\n",
    "    if use_prefix != file_prefix:\n",
    "        _USE_LOGFILE = os.path.join(WORKPATH, use_prefix + '_rate.log.txt')\n",
    "        os.system(f\"cp {_NEW_LOGFILE} {_USE_LOGFILE}\")\n",
    "\n",
    "    if os.path.exists(NIRSPEC_HOME):\n",
    "        local_path = os.path.join(NIRSPEC_HOME, outroot)\n",
    "        if not os.path.exists(local_path):\n",
    "            os.makedirs(local_path)\n",
    "\n",
    "        msg = f'cp {WORKPATH}/{use_prefix}* {local_path}/'\n",
    "        utils.log_comment(utils.LOGFILE, msg, verbose=True)\n",
    "        os.system(msg)\n",
    "\n",
    "        msg = f'sudo chown -R ec2-user {local_path}/'\n",
    "        utils.log_comment(utils.LOGFILE, msg, verbose=True)\n",
    "        os.system(msg)\n",
    "\n",
    "        if msametf is not None:\n",
    "            msg = f'cp {WORKPATH}/{msametf} {local_path}/'\n",
    "            utils.log_comment(utils.LOGFILE, msg, verbose=True)\n",
    "            os.system(msg)\n",
    "\n",
    "    utils.LOGFILE = _ORIG_LOGFILE\n",
    "\n",
    "    if clean:\n",
    "        print('Clean up')\n",
    "        files = glob.glob('*')\n",
    "        for file in files:\n",
    "            print(f'rm {file}')\n",
    "            os.remove(file)\n",
    "\n",
    "        os.chdir(HOME)\n",
    "        os.rmdir(WORKPATH)\n",
    "\n",
    "    return 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0526f9c-0ecd-48b0-96b0-3a4756e61a6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# the main run.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m LIST\u001b[38;5;241m=\u001b[39m\u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(HOME\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/*nrs*_rate.fits\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m####\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m LIST:\n\u001b[1;32m      5\u001b[0m     status \u001b[38;5;241m=\u001b[39m preprocess_nirspec_file(q,root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasq-j0100\u001b[39m\u001b[38;5;124m'\u001b[39m, clean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, extend_wavelengths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "# preprocessing the spectrum\n",
    "\n",
    "LIST=glob.glob(HOME+'/*nrs*_rate.fits') ####\n",
    "for q in LIST:\n",
    "    status = preprocess_nirspec_file(q,root='masq-j0100', clean=False, extend_wavelengths=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c07da9a9-8dad-47f8-b246-e909d594acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords for msaexp extraction\n",
    "\n",
    "group_kws = dict(\n",
    "    diffs=True, # For nod differences\n",
    "    #undo_barshadow=2, # For msaexp barshadow correction\n",
    "    min_bar=0.35, # minimum allowed value for the (inverse) bar shadow correction\n",
    "    position_key=\"y_index\",\n",
    "    trace_with_ypos=True, # Include expected y shutter offset in the trace\n",
    "    trace_from_yoffset=True,\n",
    "    #flag_profile_kwargs=None, # Turn off profile flag\n",
    "    #bad_shutter_names=[-1]\n",
    ")\n",
    "\n",
    "flag_profile_kwargs = dict(require_multiple=True, make_plot=True, grow=2, nfilt=-32)\n",
    "\n",
    "group_kws['diffs'] = True\n",
    "group_kws['flag_profile_kwargs'] = flag_profile_kwargs\n",
    "\n",
    "DRIZZLE_KWS = dict(\n",
    "    step=1,\n",
    "    with_pathloss=True,\n",
    "    wave_sample=1.05,\n",
    "    ny=13,\n",
    "    dkws=dict(oversample=16, pixfrac=0.8),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c25b021-aa39-4326-a69b-ab89f57ba736",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# this block finds all objids based on the filenames.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m all_reduced_2d \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjw04713*_phot.*.4713_*.fits\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m# you should have these files if you have run the preprocessing\u001b[39;00m\n\u001b[1;32m      5\u001b[0m all_objid \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m all_reduced_2d:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "# this block finds all objids based on the filenames.\n",
    "\n",
    "all_reduced_2d = glob.glob('jw04713*_phot.*.4713_*.fits')# you should have these files if you have run the preprocessing\n",
    "\n",
    "all_objid = []\n",
    "for f in all_reduced_2d:\n",
    "    try:\n",
    "        objid = int(f.split('.')[-2].split('_')[-1])\n",
    "    except ValueError:\n",
    "        continue\n",
    "    if not objid in all_objid:\n",
    "        all_objid.append(objid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357ddf5-e2a9-41d2-9531-e08ef75b23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for objid in all_objid:\n",
    "    \n",
    "    files = glob.glob(f'jw*_phot*{objid}.fits')\n",
    "    files.sort()\n",
    "    print(files)\n",
    "    \n",
    "    outroot = 'original%d'%objid\n",
    "    hdul, xobj = msaexp.slit_combine.extract_spectra(\n",
    "                target=str(objid),\n",
    "                root=outroot,\n",
    "                **group_kws,\n",
    "                get_xobj=True,\n",
    "                files=files,\n",
    "                drizzle_kws=DRIZZLE_KWS,\n",
    "                undo_barshadow=2\n",
    "                )\n",
    "\n",
    "    for key in xobj.keys():\n",
    "        save_xobj(xobj, './xobj_saves/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a21efc-0c3b-4f2f-b81d-83bdb004f230",
   "metadata": {},
   "source": [
    "# After running this, goes to the step 2 notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd998bde-f363-4597-b45e-a1522388f509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
